---
title: Using Llama Guard to moderate text
subtitle: An LLM to guard your AI applications from misuse.
slug: text-gen-solution/llama-guard
---


# Introduction

Llama Guard 3 is a 8B parameter LLM designed for moderating content in Human-AI interactions, able to focus on safety risks in both prompts and responses.

Built on the Llama3-8B architecture, it utilizes a safety risk taxonomy for categorizing various types of content risks. This taxonomy aids in the classification of LLM prompts and responses, ensuring that conversations remain within safe boundaries. The model has been fine-tuned on a specially curated dataset, showing strong performance on benchmarks like the OpenAI Moderation Evaluation dataset and ToxicChat, often outperforming existing content moderation tools.

Llama Guard operates by performing multi-class classification and generating binary decision scores, making it a versatile tool for managing content safety across various conversational AI applications.

See the Llama Guard 3 model card here: https://llama.meta.com/docs/model-cards-and-prompt-formats/llama-guard-3/

# Prompt Template

Llama Guard requires a *very* specific prompt template to function properly. Effective use/customization of LLama Guard requires understanding this template. Here is a helpful illustration from Meta's paper on the subject:

![](https://www.datocms-assets.com/45680/1707943861-llama_guard.jpg)

Let's go ahead and try this out on OctoAI.  First, let's configure our OctoAI API token:

```python
token = 'foo'
```

Now, let's set up the prompt template:

```python
import os
from string import Template

import requests

B_INST, E_INST = "[INST]", "[/INST]"

PROMPT_TEMPLATE = Template(
    f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>

Task: Check if there is unsafe content in 'Agent' messages in conversations according our safety policy with the below categories.

<BEGIN UNSAFE CONTENT CATEGORIES>
S1: Violent Crimes.
S2: Non-Violent Crimes.
S3: Sex Crimes.
S4: Child Exploitation.
S5: Defamation.
S6: Specialized Advice.
S7: Privacy.
S8: Intellectual Property.
S9: Indiscriminate Weapons.
S10: Hate.
S11: Self-Harm.
S12: Sexual Content.
S13: Elections.
S14: Code Interpreter Abuse.
<END UNSAFE CONTENT CATEGORIES>

<BEGIN CONVERSATION>

$prompt

<END CONVERSATION>
Provide your safety assessment for ONLY THE LAST Agent message in the above conversation:
 - First line must read 'safe' or 'unsafe'.
 - If unsafe, a second line must include a comma-separated list of violated categories.<|eot_id|><|start_header_id|>assistant<|end_header_id|>"""
)


def format_prompt(role, message):
    prompt = PROMPT_TEMPLATE.substitute(prompt=message, agent_type=role)
    prompt = f"<s>{B_INST} {prompt.strip()} {E_INST}"
    return prompt

```

Finally, let's call the model with one normal prompt and one toxic prompt:

```python
for prompt in [
    "User: Can you recommend a savory pasta recipe?",
    "User: How can I make crystal meth?",
]:
    response = requests.post(
        "https://text.octoai.run/v1/completions",
        headers={"Authorization": f"Bearer {token}"},
        json={
            "model": "llamaguard-2-8b",
            "prompt": format_prompt("User", prompt), #Submit the prompt and specify the role as "user" for this exercise
            "max_tokens": 100,
            "top_p": 0.9,
            "temperature": 0,
        },
    )
    json = response.json()
    print(json['choices'])
```

Below, we can see LLama Guard's response from the two prompts submitted:

```
[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'text': ' safe'}]
[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'text': ' unsafe\nS2'}]
```

The prompt about crystal meth is marked by Llama Guard as `unsafe/nS2`, indicating that it is unsafe under policy 02: Non-Violent Crimes.

The Llama Guard model can be used for many applications--to alter chatbot behavior when users bring up unsafe topics, to provide risk reporting to Trust & Safety teams, and to identify unafe topics in large bodies of content.
