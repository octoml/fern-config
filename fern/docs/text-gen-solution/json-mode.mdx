---
title: Using Structured Outputs (JSON mode) with Text Gen endpoints
subtitle: Ensure Text Gen outputs fit into your desired JSON schema.
slug: text-gen-solution/json-mode
---

OctoAI's Large Language Models (LLMs) can generate outputs that not only adhere to JSON format but also align with your unique schema specifications. This guide covers two approaches to JSON mode: OpenAI Compatible JSON mode for Llama-3.1-8B and 70B, and Legacy JSON mode.

**Supported models
* Llama 3.1 8B 
* Llama 3.1 70B

## OpenAI Compatible JSON mode for Llama-3.1-8B and 70B

This section covers the new JSON mode compatible with OpenAI's new response format standard, specifically for Llama-3.1-8B and 70B models.

### Setup

First, set up the OpenAI client and set it to run with OctoAI base and tokens.

```python maxLines=0
from openai import OpenAI
import os

client = OpenAI(
    base_url="https://text.octoai.run/v1",
    api_key=os.environ["OCTOAI_API_KEY"],
)

model = "meta-llama-3.1-8b-instruct"
```

### Generate JSON without adhering to any schema (json_object)

If you want the response as a JSON object but without any specific schema:

```python maxLines=0
import json

def generate_json_object():
    response = client.chat.completions.create(
        model=model,
        messages=[
            {
                "role": "system",
                "content": "Generate a JSON object, without any additional text or comments.",
            },
            {"role": "user", "content": "who won the world cup in 2022? answer in JSON"},
        ],
        max_tokens=max_tokens,
        response_format={
            "type": "json_object",
        },
        temperature=0,
    )

    content = response.choices[0].message.content
    data = json.loads(content)
    return data
```

### Generating JSON adhering to schema (without constrained decoding):

For generating JSON that adheres to a simple schema, but without strict (guarenteed) schema following (see the "strict": False below). 
This mode is faster and works on both Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct. For most use cases, it is sufficient and recommended.

```python maxLines=0
from pydantic import BaseModel
from jsonschema import validate

class Output(BaseModel):
    answer: str

def generate_json_schema_strict_false():
    schema = Output.model_json_schema()
    response = client.chat.completions.create(
        model=model,
        messages=[
            {
                "role": "system",
                "content": "Generate a JSON object, without any additional text or comments.",
            },
            {"role": "user", "content": "who won the world cup in 2022?"},
        ],
        response_format={
            "type": "json_schema",
            "json_schema": {"name": "output", "schema": schema, "strict": False},
        },
        temperature=0,
    )
    content = response.choices[0].message.content
    data = json.loads(content)
    validate(instance=data, schema=schema)
    return data
```

### Generating JSON adhering to schema (with constrained decoding):

When you need strict adherence to a JSON schema, you can activate this mode on Llama-3.1-8b-Instruct *only*. This is recommended for more complex schemas. Activating this mode can create a latency increase.

```python maxLines=0
from textwrap import dedent

math_tutor_prompt = """
    You are a helpful math tutor. You will be provided with a math problem,
    and your goal will be to output a step by step solution, along with a final answer.
    For each step, just provide the output as an equation use the explanation field to detail the reasoning.
"""

question = "how can I solve 8x + 7 = -23"

schema = {
    "type": "object",
    "properties": {
        "steps": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "explanation": {"type": "string"},
                    "output": {"type": "string"},
                },
                "required": ["explanation", "output"],
                "additionalProperties": False,
            },
        },
        "final_answer": {"type": "string"},
    },
    "required": ["steps", "final_answer"],
    "additionalProperties": False,
}

def generate_json_schema_strict_true():
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": dedent(math_tutor_prompt)},
            {"role": "user", "content": question},
        ],
        response_format={
            "type": "json_schema",
            "json_schema": {"name": "math_reasoning", "schema": schema, "strict": True},
        },
        temperature=0,
    )
    content = response.choices[0].message.content
    data = json.loads(content)
    validate(instance=data, schema=schema)
    return data
```
