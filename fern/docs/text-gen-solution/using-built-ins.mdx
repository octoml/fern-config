---
title: Calling Built-in Tools with Llama 3.1
---

In this tutorial you will learn how to use "Built-In Tools" as introduced by the Llama 3.1 family of models.

## Introduction
Function calling is a feature give LLMs the ability to consider using external code functions to respond to a user query. When the LLM triggers the use of a _tool_, it sends back a _tool_ message to the application with the name and the parameters of the function to be used. The backend application then uses this inforamtion to execute the function locally. OctoAI models support function calling already, as described in [this](https://octo.ai/docs/documentation/text-gen-solution/function-calling-using-octo-ai-models) documentation page.

## A new type of Functions
The release of the Llama 3.1 family of models introduced the concept of "Built-in Tools".
The models have enhanced support for a set of functions by default, without extra prompting or fine-tuning. In order to support these, the model was trained using a set of special tags. Using Llama built-in tools is easy with OctoAI. These are supported through our standard tool API, so you don't need to worry about any low level implementation details.

Let's take a look in the next section at how to use them. Each section will contain snippets of code that you can copy and test in your environment.

### Built-In Tools
These are the following built-in tools available in Llama 3.1 models, with their respective code tool name:
* Brave Search: `brave_search`
    * Used to perform web searches.
* Wolfram Alpha: `wolfram_alpha`
    * Used to perform complex mathematical calculations.
* Code Interpeter: `code_interpreter`
    * Used to evaluate the generated Python code.
* Photogen: `photogen`
    * Used to create images.

Built-in support only means that the models are better trained at triggering the use of these functions. The functions still need to be implemented locally. In the following sections we cover how you can trigger each of the Llama 3.1 Built-in tools.

## Brave Search Tool
The Brave Search tool gets triggered by the model when the response benefits from a web search of a given query. We will mock the function so we can get up and running quickly.

### Using the Brave Search Tool
The following snippet of code shows how to handle a chat interaction that uses the Brave Search tool:
```python
import os
from openai import OpenAI
import json


# Brave search definition
def brave_search(query: str) -> str:
    return "Search results: The weather in Boston is Sunny, with 70 degrees Fahrenheit and clear skies."


tools = [
    {"type": "function", "function": {"name": "brave_search"}},
]


client = OpenAI(
    base_url="https://text.octoai.run/v1",
    api_key=os.environ["OCTOAI_API_KEY"],
)
model = "meta-llama-3.1-8b-instruct"

messages = [
    {
        "role": "user",
        "content": "what is the current weather like in Boston?",
    },
]

# First LLM inference
completion = client.chat.completions.create(
    model=model,
    messages=messages,
    temperature=0.1,
    max_tokens=512,
    tools=tools,
)

# Append the assistant response to messages
assistant_response = completion.choices[0].message
messages.append(
    {
        "role": "assistant",
        "content": "",
        "tool_calls": completion.choices[0].message.tool_calls,
    }
)

# Handle function call from tool message
tool_call = completion.choices[0].message.tool_calls[0]
function_params = json.loads(tool_call.function.arguments)

# Compute the results (done by the backend application)
function_result = brave_search(**function_params)

# Append to the tools response
messages.append(
    {"role": "tool", "content": function_result, "tool_call_id": tool_call.id}
)

# Second LLM inference
completion = client.chat.completions.create(
    model=model,
    messages=messages,
    temperature=0.1,
    tools=tools,
    max_tokens=512,
)

print(completion.choices[0].message.content)
```

As you can see, we don't have to specify the parameters of the function, because this is a function with Built-in support. This also means that custom functions can not use the `brave_search` identifier.

You can expect a final response similar to this:
```
The current weather in Boston is sunny, with a temperature of 70 degrees Fahrenheit and clear skies.
```

## Wolfram Alpha Tool
The Wolfram Alpha tool gets triggered by the model when the response benefits from querying the Wolfram Alpha API. Let's mock the function so we can get up and running quickly.

### Using the Wolfram Alpha Tool
The following snippet of code shows how to handle a chat interaction that uses the Wolfram Alpha tool:
```python
import os
from openai import OpenAI
import json


# Wolfram Alpha definition
def wolfram_alpha(query: str) -> str:
    """
    Returns a representative response from Wolfram Alpha API
    """
    return '{"plaintext": "x = -1"}'


tools = [
    {"type": "function", "function": {"name": "wolfram_alpha"}},
]

client = OpenAI(
    base_url="https://text.octoai.run/v1",
    api_key=os.environ["OCTOAI_API_KEY"],
)
model = "meta-llama-3.1-8b-instruct"

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {
        "role": "user",
        "content": "what is the solution to the equation x^2 + 2x + 1 =0?",
    },
]

# First LLM inference
completion = client.chat.completions.create(
    model=model,
    messages=messages,
    temperature=0.1,
    max_tokens=512,
    tools=tools,
)

# Append the assistant response to messages
assistant_response = completion.choices[0].message
messages.append(
    {
        "role": "assistant",
        "content": "",
        "tool_calls": completion.choices[0].message.tool_calls,
    }
)

# Handle function call from tool message
tool_call = completion.choices[0].message.tool_calls[0]
function_params = json.loads(tool_call.function.arguments)

# Compute the results (done by the backend application)
function_result = wolfram_alpha(**function_params)


# Append to the tools response
messages.append(
    {"role": "tool", "content": function_result, "tool_call_id": tool_call.id}
)
# Second LLM inference
completion = client.chat.completions.create(
    model=model,
    messages=messages,
    temperature=0.1,
    max_tokens=512,
    tools=tools,
)

print(completion.choices[0].message.content)
```

Similarly to the Brave Search tool, we don't specify the parameters. Also custom functions can not use the `wolfram_alpha` identifier.

You can expect a final response similar to this:
```
The solution to the equation x^2 + 2x + 1 = 0 is x = -1.
```

## Code Interpreter Tool
The Code Interpreter tool gets triggered by the model when the response requires executing a snippet of Python code generated by the model itself.

### Using the Code Interpreter Tool
The following snippet of code shows how to handle a chat interaction that uses the Code Interpreter tool:
```python
import os
from openai import OpenAI
import json


# Code interpreter definition
def code_interpreter(code: str) -> str:
    return "Code executed successfully. Exit code: 0"


tools = [
    {"type": "function", "function": {"name": "code_interpreter"}},
]

client = OpenAI(
    base_url="https://text.octoai.run/v1",
    api_key=os.environ["OCTOAI_API_KEY"],
)
model = "meta-llama-3.1-8b-instruct"

messages = [
    {
        "role": "user",
        "content": "create a sine wave in python",
    },
]

# First LLM inference
completion = client.chat.completions.create(
    model=model,
    messages=messages,
    temperature=0.1,
    max_tokens=512,
    tools=tools,
)

# Append the assistant response to messages
assistant_response = completion.choices[0].message

# If there are function calls, handle the calls
if assistant_response.tool_calls:
    print("Function call detected")
    # Append the assistant response to messages
    messages.append(
        {
            "role": "assistant",
            "content": "",
            "tool_calls": assistant_response.tool_calls,
        }
    )

    # Get tool call information
    tool_call = assistant_response.tool_calls[0]
    function_name = tool_call.function.name
    function_params = json.loads(tool_call.function.arguments)

    # Print the code created
    print("=================================")
    print("Code to be executed:")
    print(function_params["code"])
    print("=================================")

    # Call the function
    function_result = code_interpreter(**function_params)

    # Append to the tools response
    messages.append({"role": "tool", "content": function_result})

    # Second LLM inference
    completion = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.1,
        max_tokens=512,
        tools=tools,
    )
    assistant_response = completion.choices[0].message

print(assistant_response.content)
```

Similarly to the Wolfram Alpha tool, one does not require to define the parameters of the function. Also, custom functions can not use the `code_interpreter` identifier.

From this request you can expect the model to generate the following code:
```python
import numpy as np
import matplotlib.pyplot as plt

# Create an array of x values from 0 to 4π
x = np.linspace(0, 4 * np.pi, 1000)

# Create a sine wave with amplitude 1 and frequency 1
y = np.sin(x)

# Create a plot of the sine wave
plt.plot(x, y)

# Add title and labels
plt.title('Sine Wave')
plt.xlabel('x')
plt.ylabel('sin(x)')

# Display the plot
plt.show()
```

With our mocked function you can expect the final response to be like this:
```
This code creates a sine wave with amplitude 1 and frequency 1, and plots it using matplotlib. The `np.linspace(0, 4 * np.pi, 1000)` function creates an array of 1000 x values from 0 to 4π, and the `np.sin(x)` function calculates the corresponding y values. The `plt.plot(x, y)` function creates the plot, and the `plt.title()`, `plt.xlabel()`, and `plt.ylabel()` functions add a title and labels to the plot. Finally, the `plt.show()` function displays the plot.
```

## Photogen Tool
The Photogen tool gets triggered by the model whenever the user request creating a picture of some form. We will mock the function to get us up and running quickly.

## Using the Photogen Tool
The following snippet of code shows how to handle a chat interaction that uses the Photogen tool:
```python
import os
from openai import OpenAI
import json


# Code interpreter definition
def photogen(query: str) -> str:
    return "Imaged generated and save to img.png"


tools = [
    {"type": "function", "function": {"name": "photogen"}},
]

client = OpenAI(
    base_url="https://text.octoai.run/v1",
    api_key=os.environ["OCTOAI_API_KEY"],
)
model = "meta-llama-3.1-8b-instruct"

messages = [
    {
        "role": "user",
        "content": "generate an image of a green field in a sunny day",
    },
]

# First LLM inference
completion = client.chat.completions.create(
    model=model,
    messages=messages,
    temperature=0.1,
    max_tokens=512,
    tools=tools,
)

# Append the assistant response to messages
assistant_response = completion.choices[0].message

# If there are function calls, handle the calls
if assistant_response.tool_calls:
    print("Function call detected")
    # Append the assistant response to messages
    messages.append(
        {
            "role": "assistant",
            "content": "",
            "tool_calls": assistant_response.tool_calls,
        }
    )

    # Get tool call information
    tool_call = assistant_response.tool_calls[0]
    function_name = tool_call.function.name
    function_params = json.loads(tool_call.function.arguments)

    # Call the function
    function_result = photogen(**function_params)

    # Append to the tools response
    messages.append({"role": "tool", "content": function_result})

    # Second LLM inference
    completion = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.1,
        max_tokens=512,
        tools=tools,
    )
    assistant_response = completion.choices[0].message

print(assistant_response.content)

```

Similarly to the Code Interpreter tool, one does not require to define the parameters of the function. Also, custom functions can not use the `photogen` identifier.

With our mocked function you can expect the final response to be like this:
```
This is an image of a green field with a few trees and a blue sky with white clouds.
```

Also note that you can use OctoAI's Image Generation service to implement the actual image generation capability in your application!

## Implementation Notes
Care must be taken in order to handle the possiblity of the model calling the `code_interpreter` at any point if any of the other buil-in tools are active. This is expected behavior and your implementation needs to handle this case.

You are in charge of doing the final implementation of these functions. This provide interesting opportunities to create new and innovative experiences. Still, we will be providing examples of implementations of these functions for the default cases in our [Text-Gen Cookbook](https://github.com/octoml/octoai-textgen-cookbook) repository soon.

## Conclusion
In this tutorial we have seen how to use Llama 3.1's Built-in Tools. You can easily take advantage of them using OctoAI's convenient API, wihtout having to worry about low-level implementation or cumbersome tool definitions.

For more examples and reference designs take a look at our [Text-Gen Cookbook](https://github.com/octoml/octoai-textgen-cookbook) repository in GitHub, or for more inspiration browse through our [demo pages](https://octo.ai/demos/).
