---
title: Text Gen TypeScript SDK
subtitle: >-
  The OctoAI Text Gen TypeScript SDK supports both the Chat Completions API and
  the Completions API.
slug: text-gen-solution/typescript-sdk
---

## At a Glance

This guide will walk you through how to use the TypeScript SDK to call our Text Gen API. The TypeScript SDK supports streaming and non-streaming inferences for both the Chat Completions API and legacy Completions API. There are also additional parameters such as `frequencyPenalty`, `maxTokens`, `presencePenalty`, etc. that can be used for finer control.

## Requirements

- Please [create an OctoAI API token](/docs/getting-started/how-to-create-octoai-access-token) if you don't have one already.
- Please also verify you've completed [TypeScript SDK Installation & Setup](/docs/typescript-sdk/installation-and-setup).
  - If you use the `OCTOAI_TOKEN` envvar for your token, you can instantiate the client with `octoai = new OctoAIClient()`, otherwise you will need to pass an API token using: `octoai = new OctoAIClient({ apiKey: process.env.OCTOAI_TOKEN })`

## Chat Completions API

### Non-Streaming Example

To make a chat completions call, you will need to provide the model you wish to call and a list of chat messages.

```typescript TypeScript
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const result = await octoai.textGen.createChatCompletion({
  model: "meta-llama-3.1-8b-instruct",
  messages: [
    {
      role: "system",
      content:
        "You are a helpful assistant. Keep your responses limited to one short paragraph if possible.",
    },
    {
      role: "user",
      content: "Write a blog about Seattle",
    },
  ],
});

console.log(result.choices[0].message.content);
// "Seattle is a vibrant and eclectic city..."
```

### Streaming Example

The above example can work great in some scenarios, but if you're dealing with larger requests or are building a highly-interactive user experience, using the streaming interface may be a better choice. The available options between non-streaming and streaming inferences are identical, but there are two main code changes needed:

- You will need to use the `createChatCompletionStream()` method instead of `createChatCompletion()`.
- Instead of grabbing the final text message from the response, you will need to loop over the individual chunks and concatenate the tokens.

```typescript TypeScript
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const stream = await octoai.textGen.createChatCompletionStream({
  model: "meta-llama-3.1-8b-instruct",
  messages: [
    {
      role: "system",
      content:
        "You are a helpful assistant. Keep your responses limited to one short paragraph if possible.",
    },
    {
      role: "user",
      content: "Write a blog about Seattle",
    },
  ],
});

let result = "";

// Loops over the returned chunks whenever they're ready
for await (const chunk of stream) {
  // The content of the first chunk can be `undefined`
  result += chunk.choices[0].delta.content ?? "";
}

console.log(result);
// "Seattle is a vibrant and eclectic city..."
```

## Completions API

The TypeScript SDK also supports the legacy Completions API with the same customization options as the Chat Completions API. The key difference between the two is that you provide a `prompt` string instead of a list of chat message objects. Much like the Chat Completions API, you can choose between non-streaming and streaming inference.

### Non-Streaming Example

```typescript TypeScript
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const response = await octoai.textGen.createCompletion({
  model: "meta-llama-3.1-8b-instruct",
  prompt: "Write a blog about Seattle",
});

console.log(response.choices[0].text);
// "Seattle is a vibrant and eclectic city..."
```

### Streaming Example

```typescript TypeScript
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const stream = await octoai.textGen.createCompletionStream({
  model: "meta-llama-3.1-8b-instruct",
  prompt: "Write a blog about Seattle",
});

let result = "";

// Loops over the returned chunks whenever they're ready
for await (const chunk of stream) {
  result += chunk.choices[0].text;
}

console.log(result);
// "Seattle is a vibrant and eclectic city..."
```
